{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65618ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "\n",
    "%pip install nltk\n",
    "%pip install bertopic\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV\n",
    "df_contratos = pd.read_csv(\"read csv exported from MAIN.IPYNB/ API REQUEST\")\n",
    "# Display the first few rows\n",
    "print(df_contratos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lemma_batch(texts, stop_words, nlp):\n",
    "    \"\"\"\n",
    "    Lemmatizes and cleans a list of texts using spaCy, removing stopwords, punctuation, and spaces.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: list of str\n",
    "        The input texts to process. For example, you can pass a column from your dataframe like:\n",
    "        df_contratos['objeto_del_contrato'].astype(str).tolist()\n",
    "    - stop_words: set\n",
    "        A set of stopwords to remove from the lemmatized tokens. You should pass your list of stopwords\n",
    "        here as a set, for example: set(stopwords.words('spanish')) or a custom set.\n",
    "    - nlp: spacy.lang object\n",
    "        The loaded spaCy language model. For example, you should load it before calling this function:\n",
    "        nlp = spacy.load(\"es_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "    Returns:\n",
    "    - cleaned_texts: list of str\n",
    "        The processed texts, where each text is a string of lemmatized tokens separated by spaces,\n",
    "        with stopwords, punctuation, and spaces removed.\n",
    "\n",
    "    Where to place everything:\n",
    "    1. Load your dataframe (df_contratos) as you already do.\n",
    "    2. Prepare your stopwords set, e.g.:\n",
    "           from nltk.corpus import stopwords\n",
    "           stop_words = set(stopwords.words('spanish'))\n",
    "           # Optionally, add your own custom stopwords:\n",
    "           stop_words.update(['palabra1', 'palabra2', ...])\n",
    "    3. Load the spaCy model:\n",
    "           import spacy\n",
    "           nlp = spacy.load(\"es_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "    4. Extract the texts from your dataframe:\n",
    "           texts = df_contratos['objeto_del_contrato'].astype(str).tolist()\n",
    "    5. Call this function:\n",
    "           cleaned = clean_lemma_batch(texts, stop_words, nlp)\n",
    "\n",
    "    You do NOT need to put the dataframe inside this function. You only pass the column you want to process as a list of strings.\n",
    "\n",
    "    Example usage:\n",
    "        import spacy\n",
    "        from nltk.corpus import stopwords\n",
    "\n",
    "        stop_words = set(stopwords.words('spanish'))\n",
    "        stop_words.update(['palabra1', 'palabra2'])  # add your custom stopwords here\n",
    "        nlp = spacy.load(\"es_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "        texts = df_contratos['objeto_del_contrato'].astype(str).tolist()\n",
    "        cleaned = clean_lemma_batch(texts, stop_words, nlp)\n",
    "    \"\"\"\n",
    "    cleaned_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=1000, disable=[\"parser\", \"ner\"]):\n",
    "        tokens = [\n",
    "            token.lemma_ for token in doc\n",
    "            if token.lemma_ not in stop_words\n",
    "            and not token.is_punct\n",
    "            and not token.is_space\n",
    "        ]\n",
    "        cleaned_texts.append(' '.join(tokens))\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Define the function to clean and lemmatize a batch of texts\n",
    "def clean_lemma_batch(texts, stop_words, nlp):\n",
    "    cleaned_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=1000, disable=[\"parser\", \"ner\"]):\n",
    "        tokens = [\n",
    "            token.lemma_ for token in doc\n",
    "            if token.lemma_ not in stop_words\n",
    "            and not token.is_punct\n",
    "            and not token.is_space\n",
    "        ]\n",
    "        cleaned_texts.append(' '.join(tokens))\n",
    "    return cleaned_texts\n",
    "\n",
    "# Prepare stopwords and custom contract stopwords\n",
    "contract_stopwords = {\n",
    "    'prestacion', 'servicio', 'proceso', 'contratista', 'acuerdo', 'ejecutar',\n",
    "    'condicion', 'empresa', 'nuevo', 'institucional', 'entidad', 'area',\n",
    "    'eficaz', 'eficiente', 'municipio', 'nivel', 'centro', 'ejecuci칩n',\n",
    "    'prestaci칩n', 'objeto', 'dentro', 'requerido', 'diferente', 'poner',\n",
    "    'realizar',\n",
    "    'apoyo', 'departamental', 'contrato', 'laboral', 'actividad', 'cuenta',\n",
    "    'profesional', 'contratar', 'especializado', 'prestar', '치rea',\n",
    "    'subproceso', 'comprometer', 'garantizar', 'requerir', 'adelantar',\n",
    "    'local', 'primero', 'conformidad', 'responsabilidad', 'efectivo',\n",
    "    'disposici칩n', 'forma', 'propuesta', 'bajo', 'oportuno', 'tiempo', 'autonom칤a'\n",
    "}\n",
    "\n",
    "# Example usage (to be run outside this cell, not inside the function):\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('spanish'))\n",
    "# stop_words.update(contract_stopwords)\n",
    "# nlp = spacy.load(\"es_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "# texts = df_contratos['objeto_del_contrato'].astype(str).tolist()\n",
    "# cleaned = clean_lemma_batch(texts, stop_words, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39035255",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "stop_words.update(contract_stopwords)\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=[\"ner\", \"parser\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_in_chunks(df_contratos, chunk_size=10000):\n",
    "    total_chunks = len(df_contratos) // chunk_size + 1\n",
    "    cleaned_results = []\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, len(df_contratos))\n",
    "        chunk = df_contratos.iloc[start:end].copy()\n",
    "        print(f\"游댳 Processing chunk {i+1}/{total_chunks} ({end-start} rows)\")\n",
    "\n",
    "        texts = chunk['objeto_del_contrato'].astype(str).tolist()\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b79189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_in_chunks(df, stop_words, nlp, chunk_size=10000):\n",
    "    total_chunks = len(df) // chunk_size + 1\n",
    "    cleaned_results = []\n",
    "\n",
    "    for i in range(total_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, len(df))\n",
    "        chunk = df.iloc[start:end].copy()\n",
    "        print(f\"游댳 Processing chunk {i+1}/{total_chunks} ({end-start} rows)\")\n",
    "\n",
    "        texts = chunk['objeto_del_contrato'].astype(str).tolist()\n",
    "        chunk['objeto_clean_lemma'] = clean_lemma_batch(texts, stop_words, nlp)\n",
    "\n",
    "        cleaned_results.append(chunk)\n",
    "\n",
    "    return pd.concat(cleaned_results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc76552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = process_dataframe_in_chunks(df_contratos, stop_words, nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85697354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model once\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic.load(\"my_bertopic_modelv07\",embedding_model=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# Transform in batches if needed\n",
    "docs = df_cleaned['objeto_clean_lemma'].tolist()\n",
    "topics, probs = topic_model.transform(docs)\n",
    "\n",
    "df_cleaned['predicted_topic'] = topics\n",
    "df_cleaned['topic_probability'] = probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame with topics to CSV\n",
    "df_cleaned.to_csv(\"contratos_2024_labeled.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e20967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame with topics to a pickle file\n",
    "df_cleaned.to_pickle(\"contratos_2024_labeled.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
